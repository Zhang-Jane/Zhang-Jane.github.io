---
title: python面试总结
tags: 面试
categories: 面试
abbrlink: 5fbea8d0
date: 2020-10-26 14:48:49
---
# 关于python后端开发工程师

https://blog.csdn.net/AyoCross/article/details/105294560

# 关于python相关面试题

https://github.com/taizilongxu/interview_python

# python基础相关

## python基础的数据结构

### 集合

- 可变的，无序的，不重复的。

- 定义用set（），不能{}。

- set里面元素必须可以hash，里面的元素不可以索引。

### 元祖

- 有序的元素的集合
- 元祖是不可变对象
- 元祖的定义(1,)，不能(1)
- 元祖不可变，只有读方法

### 字典

- 可变的，无序的
- key不重复且可hash
- 定义
    - dict()
    - dict(**kwargs)
    - dict(二元结构)
    - dict.fromkeys(iterable, values)

### 出题

#### 关于定义：

```
题目1：
A: tuple: a = (1)
B: set: b = {}
C: dict: c = dict(a="b")
D: dict: d = dict(a=(1, ))
E: dict: e = dict(a=[1,2])

变形题目2：
A: dict: b = dict{(1,2):3}
B: dict: c = dict(a=(1,2))
C: dict: d = dict(a={1,2}
D: dict: e = dict({1,2}:3)
```

```
变形题目3：
A: dict: c = dict(zip([1,2], (3,4)))
B: dict: d = dict(([1,2], [3,4]))
C: dict: a = dict((1, 2), (3,4))
D: dict: e = dict.fromkeys([1,2,3,4], 1)

变形题目4：
A: dict: c = dict.fromkeys([1,2], (3,4)))
B: dict: d = dict.fromkeys({1:2}, (3,4)))
C: dict: a = dict.fromkeys("12", "34")
D: dict: a = dict.fromkeys(1, "34")
E: dict: e = dict.fromkeys([1,2,3,4], 1)
```

#### 关于操作：

```
A: (1, 2) + (1, 2)
B: (1, 2) + [1, 2]
C: {1, 2} + {1, 2}
D: [1, 2] + [[1,2], 2]
```

```
A: a, b = 1, 2 => a = 1,b = 2
B: *a, b = 1, 2 => a = (1, ), b = 2
C: a = 1, 2 => a = (1, 2)
D: a, *_ = 1, 2 => a = 1
```

```
A: a = 1, 2, 3 => a = (1,2,3)
B: a, *_, b = 1,2,3,4 => a = 1, _ = (2, 3), b = 4 
C: *_, a, b = 1,2,3,4 => _ = [1, 2], a = 3, b= 4
D: *_, a, b = (1,2,3,(4,5)) => _ = [1, 2], a = 3, b = (4, 5)
```

```
A: *_, a, b = (1,2,3,(4,5),6,[7,8]) => _ = [1,2,3,(4,5)]
B: *_, a, b = (1,2) => _ = [1]
C: a, *_, b = 1,2,3 => _ = [2]
D: a, b, *_ = 1,2,3 => _ = [3]

```

```
A: a, b, [] = 1, 2, 3
B: a, b, *[a] = 1, 2, 3
C: a, b, **{"a":2} = 1, 2, 3
D: a, b, c, x = 1, 2, 3, lambda x:x

```

#### 关于函数传参

```
A: demo(x, y, *args, **kw):
B: def demo(*args, x, **kw):
C: def demo(*args, y,  **kw):
D: def demo(*args, **kw, y):

```

```
A: def demo(*args, y,  **kw):
	print(args, y, kw)
demo(1, 2, y=3)  

B: def demo(*args, y,  **kw):
	print(args, y, kw)
demo(1,2, y=3, z=4)

C:def demo(*args, y,  **kw):
	print(args, y, kw)
demo([1,2], 1, 2, 3, y=5, z=4)

D:def demo(*args, y, **kw):
	print(args, y, kw)
demo([1,2], 1, a=2, 3, y=5, z=4)
```

```
def demo(x, *args, y, **kw):
	print(x, args, y, kw)
demo([1,2], 1, a=2, 3, y=5, z=4)
上述的代码报错吗，不报错答案是什么？
```

#### 关于生成器

```
def coro(a):
    print("-> start : a = ", a)
    b = yield a+1
    print("-> recive : b ", b)
    c = yield a+b
    print("-> recive : c ", c)
    d = yield
    print("-> recive : d ", d)
    yield
a = coro(1)
next(a)
next(a)
输出怎么样的结果
```

```
def coro(a):
    print("-> start : a = ", a)
    b = yield a+1
    print("-> recive : b ", b)
    c = yield a+b
    print("-> recive : c ", c)
    d = yield
    print("-> recive : d ", d)
    yield
a = coro(1)
next(a)
a.send(2)
next(a)
输出怎么样的结果
```

```
def coro(a):
    print("-> start : a = ", a)
    b = yield a+1
    print("-> recive : b ", b)
    c = yield a+b
    print("-> recive : c ", c)
    d = yield
    print("-> recive : d ", d)
    yield
a = coro(1)
next(a)
a.send(2)
a.send(3)
输出怎么样的结果
```

```
def coro(a):
    print("-> start : a = ", a)
    b = yield a+1
    print("-> recive : b ", b)
    c = yield a+b
    print("-> recive : c ", c)
    d = yield
    print("-> recive : d ", d)
    yield
a = coro(1)
next(a)
a.send(2)
next(a)
a.send(3)
next(a)
输出怎么样的结果
```

## Python 直接赋值、浅拷贝和深度拷贝解析

- **直接赋值：**其实就是对象的引用（别名）。
- **浅拷贝(copy)：**拷贝父对象，不会拷贝对象的内部的子对象。
- **深拷贝(deepcopy)：** copy 模块的 deepcopy 方法，完全拷贝了父对象及其子对象

```python
# 浅拷贝
alist = [1, 2, 3, ["a","b"]]
clist = copy.copy(alist)
clist[1] = 5
print(alist)
print(clist)
alist[3][0] = 1
print(alist)
print(clist)

[1, 2, 3, ['a', 'b']]
[1, 5, 3, ['a', 'b']]
[1, 2, 3, [1, 'b']]
[1, 5, 3, [1, 'b']]

# 说明，c浅拷贝的a的对象引用，当c改变的时候不会改变a，当a改变的时候会影响c
    
# 深拷贝
alist = [1, 2, 3, ["a","b"]]
clist = copy.deepcopy(alist)
clist[1] = 5
print(alist)
print(clist)
alist[3][0] = 1
print(alist)
print(clist)

[1, 2, 3, ['a', 'b']]
[1, 5, 3, ['a', 'b']]
[1, 2, 3, [1, 'b']]
[1, 5, 3, ['a', 'b']]

# 赋值语句
alist = [1, 2, 3, ["a","b"]]
clist = alist
clist[1] = 5
print(alist)
print(clist)
alist[3][0] = 1
print(alist)
print(clist)

[1, 5, 3, ['a', 'b']]
[1, 5, 3, ['a', 'b']]
[1, 5, 3, [1, 'b']]
[1, 5, 3, [1, 'b']]
```



## 类 class 和元类 metaclass 的有什么区别？

类和元类的区别：

- 类的目的是用来来创建实例对象
- 元类的目的是用来创建类（元类是最底层的东西，是所有类的父类）

类：由于类是由元类创建得到的，因此我们可以认为类的本质是一个对象，既然类是一个对象，我们就可以像操作对象一样操作一个类：可以将类赋值给一个变量、可以复制这个类、可以为其添加属性等等。

元类我们实际开发使用并不需要考虑和使用它。

## 实例方法、类方法和静态方法有什么不同？

实例方法、类方法和静态方法，都是属于类的方法，不同点就是传入的参数和调用的方式。

三种方法传入的参数分别是：self（实例化对象）、cls（类本身）、无参数

三种方法的调用方式：

- 实例方法：由对象调用；至少一个 self 参数；执行实例方法时，自动将调用该方法的实例化对象赋值给 self；实例化方法中可以修改实例的属性
- 类方法：由类调用；至少一个 cls 参数；执行类方法时，自动将调用该方法的类赋值给 cls；类方法中可以修改类的属性
- 静态方法：由类调用；无默认参数；内部使用；
- 静态方法特殊性：静态方法不需要传入实例或者类，它是用来被类内部普通方法或类方法使用的一种方法

## 类中的 property 属性有什么作用？

- property 属性的定义有两种方式：装饰器和类属性
- 通过调用 property 属性，可以简化获取数据的流程
- property 属性目的：优化代码，简化代码，调用 @property 装饰的方法可以获得一个属性，调用该方法写法和调用实例属性一样

参考代码：

```python
class Person():

    def __init__(self, name):
        self.name = name

    # 定义一个属性的特殊方法,并且参数只有一个self，有一个返回值
    @property
    def age(self):
        return 28

# 实例化对象
obj = Person("Felix")
# 调用实例属性
name = obj.name
print(name)

# 调用property属性，方法后面的括号不能写
# 这样就类似于调用实例中的一个属性
ret = obj.age # 结果就是方法的返回值
print(ret)

# Felix
# 28
```

## 类如何才能支持比较操作？

- 使用运算符重载
- @total_ordering 类装饰器可以根据类中定义的，小于方法和等于方法自己进行推测

参考代码：

```python
class Rectangle(object):

    def __init__(self, w, h):
        self.w = w
        self.h = h
    def area(self):
        return self.w * self.h
    # < 运算符重载
    def __lt__(self, other):
        print('in__lt__')
        return self.area() < other.area()
    # <= 运算符重载
    def __le__(self, other):
        print('in__le__')
        return self.area() <= other.area()

# 创建两个矩形实例
r1 = Rectangle(2, 8)
r2 = Rectangle(4, 4)
print(r1 < r2)
print('*' * 50)
```

## hasattr()、getattr()、setattr()、delattr()分别有什么作用？

- hasattr(object,name)：判断一个函数是否有name属性或者方法，返回布尔值
- getattr(object, name)：获取对象的属性
- setattr(object, name, values)：赋值给对象的属性，如果该属性不存在，则先创建该对象，然后赋值
- delattr(object, name, values)：删除对象的属性

```python
class func(object):
    name = 'Felix'
    def run(self):
        return "hello, python"

# 判断属性或方法是否存在
func = func()
res = hasattr(func, "name") # 判断对象是否有name属性，True
res = hasattr(func, "run") # 判断对象是否有run方法，True
res = hasattr(func, "age") # 判断对象是否有age属性，False

# 获取属性
func = func()
getattr(func, "name") # 获取name属性，Felix
getattr(func, "age")  # 获取不存在的属性，出现错误

# 设置属性
func= func()
res = hasattr(func, "age") # 判断age属性是否存在，False
print(res)
setattr(func, "age", 18) # 对age属性进行赋值，无返回值
res1 = hasattr(func, "age") # 再次判断属性是否存在，True

# 删除属性
delattr(func, 'name') # 返回值None
```

## 大文件只需读取部分内容，或者避免读取时候内存不足的解决方法？

```python
# islice返回一个生成器函数，进行切片操作,取出索引为101到105的值
with open('filetest.txt', 'r') as f:
    for line in islice(f, 101, 105):
        print(line)
    for line in islice(f, 5): #只给一个参数，指定的是结束的位置
        print(line)
```

## 什么是上下文？with 上下文管理器原理？

with 方法常用于打开文件，使用 with 方法打开文件后可以自动关闭文件，即使打开或者使用文件时出现了错误，文件也可以正常关闭。

什么是上下文（context）？

- context 其实说白了，和一篇文章中的上下文是一个意思，在通俗一点，我觉得叫环境更好。
- 上下文虽然叫上下文，但是程序里面一般都只有上文而已，只是为了叫的好听叫上下文。
- 进程中断在操作系统中是有上有下的，不过不这个高深的问题就不要深究了
- 任何实现了 __enter__() 和 __exit__() 方法的对象都可以称之为上下文管理器，上下文管理器对象可以使用with关键字。显然，文件（file）对象也实现了上下文管理器。

那么文件对象是如何实现这两个方法的呢？我们可以模拟实现一个自己的文件类，让该类实现 __enter__() 和 __exit__() 方法。

```python
# 文件管理器
class File():

    # 初始化方法
    def __init__(self, filename, mode):
        self.filename = filename
        self.mode = mode

    # 打开文件的方法，返回我们打开的对象，也就是传入的文件
    def __enter__(self):
        print("entering")
        self.f = open(self.filename, self.mode)
        return self.f

    # 关闭文件的方法，不用我们自己手动去关闭文件对象了
    def __exit__(self, *args):
        print("will exit")
        self.f.close()

with File('test.txt', 'w') as f:
    print("writing")
    f.write('hello, world')
```

## 什么是全缓冲、行缓冲和无缓冲？

- 全缓冲：Python 中默认的缓冲区是 4096 字节，当缓冲区里面的空间占满后，就将数据写入到磁盘中。
- 行缓冲：当写入的数据，每遇到换行符，就将缓冲中的数据写入到磁盘中。
- 无缓冲：无缓冲，顾名思义就是一写入数据，就将数据写入到磁盘中。
- 三种缓冲，可以通过 buffering 参数进行设置。

```python
# -*- coding:utf-8 -*-

# python中默认的缓冲区(全缓冲)是4096字节
# buffering可以设置缓冲字节大小，当写入的数据超过设置值，才会写入到文件中
f = open('001_测试缓冲案例文件.txt', 'w', buffering=2048)
# 写入3个字节，打开txt文件为空
f.write('abc')
# 写入2045个字节，总共2048个，还是为空
f.write('*' * 2045)
# 此时我们在写入一个字节，就由缓冲存储到磁盘了，此时打开txt文件就可以看见数据了


# 行缓冲： buffering=1
# f = open('001_测试缓冲案例文件.txt', 'w', buffering=1)
# f.write('abc')
# 只要遇到换行符，就将缓存存到磁盘
# f.write('\n')


# 无缓冲：buffering=0
# 写入数据就直接存储到磁盘
```

## 什么是序列化和反序列化？JSON 序列化时常用的四个函数是什么？

**什么是序列化**：我们程序中的变量和对象（比如文字、图片等内容），在传输的时候需要使用二进制数据，将这些变量或对象转换为二进制数据的过程，就是序列化。

**什么是反序列化**：反序列化就是序列化的逆过程，把获取的二进制数据重建为变量或对象。实际序列化和反序列化就是二进制数据和原始数据格式之间的一个转换过程。

**JSON 中常用的四个函数**：

- json.dump：将数据序列化到文件中
- json.load：将文件中的内容反序列化读取出来
- json.dumps：将 Python 格式转化为 JSON 的字符串形式（序列化）
- json.loads：将 JSON 的字符串格式转换为 Python 的数据格式（反序列化）
- 上面容易混淆，记住 load 是下载的意思，就是将 JSON 的读取出来或者转换为 Python 的数据格式；dump 是倾倒，意思就是把数据放进去，正好和 load 相反。

注意：标准 JSON 格式数据要使用双引号。

## Python 的内存管理机制是什么？

内存管理机制：引用计数、垃圾回收、内存池。

引用计数：引用计数是对变量引用次数的一种标记手段，对象被引用时，我们对其计数增加 1，当该对象不被引用时，我们对其计数减去 1，如果计数变成了 0，说明该对象没有被引用，此时我们就可以删除该对象。

垃圾回收：Python 中有三种垃圾回收机制，其中引用计数为主，还有标记清除和分代回收。

内存池：

Python引用了一个内存池(memory pool)机制，即Pymalloc机制(malloc:n.分配内存)，用于管理对小块内存的申请和释放
内存池（memory pool）的概念：
当创建大量消耗小内存的对象时，频繁调用new/malloc会导致大量的内存碎片，致使效率降低。内存池的概念就是  预先在内存中申请一定数量的，大小相等 的内存块留作备用，当有新的内存需求时，就先从内存池中分配内存给这个需求，不够了之后再申请新的内存。这样做最显著的优势就是能够减少内存碎片，提升效率。
内存池的实现方式有很多，性能和适用范围也不一样。
Python中的内存管理机制——Pymalloc：
Python中的内存管理机制都有两套实现，一套是针对小对象，就是大小小于256bits时,pymalloc会在内存池中申请内存空间；当大于256bits，则会直接执行new/malloc的行为来申请内存空间。
关于释放内存方面，当一个对象的引用计数变为0时，Python就会调用它的析构函数。在析构时，也采用了内存池机制，从内存池来的内存会被归还到内存池中，以避免频繁地释放动作

## 常用库，xpath，正则表达式

### 正则表达式

##### **match、search 和 findall 有什么区别？**

- match 从字符串的开始进行匹配，如果字符串第一个字符不符合匹配规则，则匹配失败，函数返回 None 值；
- search 从字符串左侧开始，然后向右匹配字符串，当找到第一个匹配，匹配结束；
- findall 查找整个字符串，返回所有的匹配结果，匹配结果是一个列表。

### 正则表达式的 ()、[]、{} 分别代表什么意思？

- `()`：匹配的的字符串进行分组，目的是为了提取匹配的字符串。表达式中有几个 `()` 就有几个相应的匹配字符串, 一个 `()` 代表一个组。
- `[]`：定义匹配的字符范围，匹配多个数字多个字母等, 匹配中括号任何一个都可以，类似或或或… 比如 `[a-zA-Z0-9]` 表示匹配字母和数字。`[\s*]` 表示空格或者 `*` 号。（注意：中括号里面的所有表达式只匹配一个就结束，中括号只代表一个字符。要匹配多个后面使用 `+` 号）
- `{}`：一般用来定义匹配的长度，只限制 `{}` 它前面的一个字符，比如 `\d{3}` 表示匹配三个数字，`\d{1,3}` 表示匹配一到三个数字，包括 1 和 3。

### 正则表达式中的 `.\*`、`.+`、`.\*?`、`.+?` 有什么区别？

- `*` 匹配 0 个或多个的表达式，贪婪模式
- `+` 匹配 1 个或多个的表达式，非贪婪
- `?` 匹配 0 个或 1 个由问好前面字符或者表达式，非贪婪方式
- `.*`：贪婪匹配，找到满足条件的最大匹配
- `.+`：贪婪匹配，找到满足条件的最大匹配
- `.*?`：非贪婪，找到满足条件的最小匹配
- `.+?`：非贪婪，找到满足条件的最小匹配

# http协议相关

## HTTP和HTTPS的区别

http=tcp+80，https=tcp+443

HTTP：是互联网上应用最为广泛的一种网络协议，是一个客户端和服务器端请求和应答的标准（TCP），用于从WWW服务器传输超文本到本地浏览器的传输协议，它可以使浏览器更加高效，使网络传输减少。

HTTPS：是以安全为目标的HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。

HTTPS协议的主要作用可以分为两种：一种是建立一个信息安全通道，来保证数据传输的安全；另一种就是确认网站的真实性。

客户端在使用HTTPS方式与Web服务器通信时有以下几个步骤，如图所示。

　　（1）客户使用https的URL访问Web服务器，要求与Web服务器建立SSL连接。

　　（2）Web服务器收到客户端请求后，会将网站的证书信息（证书中包含公钥）传送一份给客户端。

　　（3）客户端的浏览器与Web服务器开始协商SSL连接的安全等级，也就是信息加密的等级。

　　（4）客户端的浏览器根据双方同意的安全等级，建立会话密钥，然后利用网站的公钥将会话密钥加密，并传送给网站。

　　（5）Web服务器利用自己的私钥解密出会话密钥。

　　（6）Web服务器利用会话密钥加密与客户端之间的通信。



## HTTP1.0和1.1的区别

https://www.jianshu.com/p/52d86558ca57

# scrapy的框架

## 基本组件

Scrapy 主要有 5 大部件和 2 个中间件。

- **Scrapy Engine（引擎）**：负责 Spider、ItemPipeline、Downloader、Scheduler 中间的通讯，信号、数据传递等
- **Scheduler（调度器）**：它负责接受引擎发送过来的 Request 请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎
- **Downloader（下载器）**：负责下载 Scrapy Engine（引擎）发送的所有 Requests 请求，并将其获取到的 Responses 交还给 Scrapy Engine（引擎），由引擎交给 Spider 来处理
- **Spider（爬虫）**：它负责处理所有 Responses，从中分析提取数据，获取 Item 字段需要的数据，并将需要跟进的 URL 提交给引擎，再次进入 Scheduler（调度器）
- **Item Pipeline（管道）**：它负责处理 Spider 中获取到的 Item，并进行进行后期处理（详细分析、过滤、存储等）的地方
- **Downloader Middlewares（下载中间件）**：你可以当作是一个可以自定义扩展下载功能的组件
- **Spider Middlewares（Spider 中间件）**：你可以理解为是一个可以自定扩展和操作引擎和 Spider 中间通信的功能组件（比如进入 Spider 的 Responses，和从 Spider 出去的 Requests）

## Scrapy 是如何实现去重的？指纹去重是什么？

- Scrapy 配置文件里面有个 dont_filter 参数，设置 False 就是开启去重，默认值就是 False；
- 调度器会根据每一次的 URL 请求，对请求的相关信息进行加密，加密后的信息存储在一个集合之中，如果有新的请求，加密后和集合中已有的信息进行对比，如果已经存在，则改请求就淘汰，以此实现去重。

## Scrapy 项目中的常用命令有哪些？

```bash
scrapy startproject（创建一个爬虫项目）
scrapy crawl XX（运行 XXX 爬虫项目）
scrapy shell http://www.hao123.cn（调试网址为 http://www.hao123.cn 的网站）
scrapy version 查看版本信息
scrapy list 查看爬虫信息，显示目录所有的爬虫
```

## 描述一下 Scrapy 爬取一个网站的工作流程？

```
1. Engine获得从爬行器中爬行的初始请求。
2. Engine在调度程序中调度请求，并请求下一次抓取请求。
3. 调度程序将下一个请求返回到引擎。
4. 引擎将请求发送到下载器，通过下载器中间件（请参阅process_request()）。
5. 页面下载完成后，下载器生成一个响应(带有该页面)并将其发送给引擎，通过下载器中间件(请参阅process_response())。
6. 引擎从下载加载程序接收响应，并将其发送给Spider进行处理，并通过Spider中间件(请参阅process_spider_input())。
7. Spider处理响应，并向引擎返回报废的项和新请求(要跟踪的)，通过Spider中间件(请参阅process_spider_output())。
8. 引擎将已处理的项目发送到项目管道，然后将已处理的请求发送到调度程序，并请求可能的下一个请求进行抓取。
9. 这个过程重复(从第1步)，直到调度程序不再发出请求。

简单总结：
start_request（引擎）>调度器>引擎>(下载中间件)>下载器>(下载中间件)>引擎>(爬虫中间件)>爬虫>(爬虫中间件)>引擎>管道&&调度器> more
```

## Spider、CrawlSpider、XMLFeedSpider 和 RedisSpider 有什么区别？

- Spider：scrapy.Spider, 是所有 Spider 的基类，它是最基础的爬虫，所有的 spider 都会继承 scrapy.Spider。它提供了 start_requests() 的默认实现，读取并请求 spider 属性中的 start_urls，并根据返回的 response 调用 spider 中的 parse 方法。
- CrawlSpider：scrapy.spiders.CrawlSpider，规则爬虫，提供了一个新的属性 rules，该属性是一个包含一个或多个 Rule 对象的集合，每个 Rule 对爬取网站的动作定义了特定的规则。
- XMLFeedSpider：scrapy.spiders.XMLFeedSpider 设计用于通过迭代各个节点来分析 XML 源。
- RedisSpider：scrapy_redis.spiders.RedisSpider,scrapy-redis 是 Scrapy 框架基于 Redis 数据库的组件，用于 scrapy 项目的分布式开发和部署，可以方便的进行分布式爬取和数据处理。

## Scrapy如何爬取属性在不同页面的item呢

```
通过response的meta来进行传递，字典键值对。不但是item，就算是一些自定义参数也可以传。

Request.meta special keys
The Request.meta attribute can contain any arbitrary data, but there are some special keys recognized by Scrapy and its built-in extensions.

Those are:

dont_redirect

dont_retry

handle_httpstatus_list

handle_httpstatus_all

dont_merge_cookies

cookiejar

dont_cache

redirect_reasons

redirect_urls

bindaddress

dont_obey_robotstxt

download_timeout

download_maxsize

download_latency

download_fail_on_dataloss

proxy

ftp_user (See FTP_USER for more info)

ftp_password (See FTP_PASSWORD for more info)

referrer_policy

max_retry_times
```

## Scrapy爬虫有内存泄露/内存爆了/栈溢出了，怎么办?

https://docs.scrapy.org/en/latest/topics/leaks.html?highlight=trackref

1. 首先检查编写的中间件，管道(pipeline) 或扩展。例如，在 spider_opened 中分配资源但在 spider_closed 中没有释放它们。
2. 检查是否出现下载速度和管道存储速度是否平衡，如果差距很大，会造成队列积压。
3. 使用 trackref 在telNet下调试内存泄露，具体问题具体分析，其追踪了所有活动(live)的Request, Item及Selector对象的引用。
4. 使用 get_oldest() 方法来看都是什么鬼东西在那卡半天，找出来单独测试然后寻找解决办法。
5. 使用Pympler模块监测python对象

## cookiejar使用以及作用

There is support for keeping multiple cookie sessions per spider by using the [`cookiejar`](https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#std-reqmeta-cookiejar) Request meta key. By default it uses a single cookie jar (session), but you can pass an identifier to use different ones.

For example:

```
for i, url in enumerate(urls):
    yield scrapy.Request(url, meta={'cookiejar': i},
        callback=self.parse_page)
```

Keep in mind that the [`cookiejar`](https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#std-reqmeta-cookiejar) meta key is not “sticky”. You need to keep passing it along on subsequent requests. For example:

```
def parse_page(self, response):
    # do some processing
    return scrapy.Request("http://www.example.com/otherpage",
        meta={'cookiejar': response.meta['cookiejar']},
        callback=self.parse_other_page)
```

## ItemLoader的用法

https://blog.csdn.net/zwq912318834/article/details/79530828

# kafka消息队列

在Kafka有几个比较重要的概念：

- broker

    用于标识每一个Kafka服务，当然同一台服务器上可以开多个broker,只要他们的broker id不相同即可

- Topic

    消息主题，从逻辑上区分不同的消息类型

- Partition

    用于存放消息的队列，存放的消息都是有序的，同一主题可以分多个partition，如分多个partiton时，同样会以如partition1存放1,3,5消息,partition2存放2,4,6消息。

- Produce

    消息生产者，生产消息，可指定向哪个topic，topic哪个分区中生成消息。

- Consumer

    消息消费者，消费消息，同一消息只能被同一个consumer group中的consumer所消费。consumer是通过offset进行标识消息被消费的位置。当然consumer的个数取决于此topic所划分的partition，如同一group中的consumer个数大于partition的个数，多出的consumer将不会处理消息。

1. 在订阅消费模式下（c.subscribe(['mytopic'])），kafka保证每条消息在同一个Consumer Group中只会被某一个Consumer消费，就是同一个组下面的Consumer，A消费了，B就不会消费。
2. 一个消费者可以同时消费多个topic
3. 消费者的数量应不多于该topic分区的数量，否则多余的消费者必定无法收到消息
4. 多个消费者，消费同一个分区，消费到的数据由生产的发往那个分区决定的
5. 在设置一个分区的时候，生产和消费的顺序是一致的，因为只有一个分区。而设置多个分区时，消费获取分区时候是无序的，因此导致生产和消费之间顺序不一致。**分区之间是无序的，分区内的消息是有序的。**
6. 为了解决多个分区导致的生产与消费之间的顺序不一致，我们需要向指定的分区生产以及消费

# 关系型数据库和非关系型数据库的区别

### 关系型数据库

采用了关系模型来组织的数据库。关系模型指的是二维的表格。

- **数据库**：包括一个或多个表
- **表（关系 Relation）**：是以列和行的形式组织起来的数据的集合
- **列（属性 Attribute）**：在数据库中经常被称为字段
- **行（值组 Tuple）**：在数据库中经常被称为记录

#### 关系型数据库优点

- **事务一致性**：通过事务处理保持数据的一致性
- **复杂查询**：支持SQL，可以进行 JOIN 等复杂查询
- **容易理解**：二维表结构是非常贴近逻辑世界的一个概念，关系模型相对网状、层次等其他模型来说更容易理解
- **使用方便**：通用的 SQL 语言使得操作关系型数据库非常方便
- **易于维护**：丰富的完整性(实体完整性、参照完整性和用户定义的完整性)大大减低了数据冗余和数据不一致的概率

#### 关系型数据库缺点

- **读写性能**：在数据量达到一定规模时，由于关系型数据库的系统逻辑非常复杂，为了维护一致性，使得其非常容易发生死锁等的并发问题，所以导致其读写速度下滑非常严重
- **表结构更新**：表结构可以在被定义之后更新，但是如果有比较大的结构变更的话就会变得比较复杂
- **高并发**：网站的用户并发性非常高，往往达到每秒上万次读写请求，对于传统关系型数据库来说，硬盘I/O是一个很大的瓶颈
- **海量数据**：对于关系型数据库来说，在一张包含海量数据的表中查询，效率是非常低的

### 非关系型数据库

非关系型数据库（NoSQL）是对不同于传统的关系数据库的数据库管理系统的统称。

可以参考如下：

https://juejin.im/post/6844903654869172232

# Mysql，redis

## mysql

### Innodb和Myisam的区别

```text
Innodb存储引擎    mysql5.6之后的默认的存储引擎
数据和索引存储在一起 2个文件
    数据索引\表结构
数据持久化
支持事务   : 为了保证数据的完整性,将多个操作变成原子性操作   : 保持数据安全
支持行级锁 : 修改的行少的时候使用                          : 修改数据频繁的操作
支持表级锁 : 批量修改多行的时候使用                        : 对于大量数据的同时修改
支持外键   : 约束两张表中的关联字段不能随意的添加\删除      : 能够降低数据增删改的出错率


Myisam存储引擎    mysql5.5之前的默认的存储引擎
数据和索引不存储在一起  3个文件
    数据\索引\表结构
数据持久化
只支持表锁

InnoDB 是聚集索引，MyISAM 是非聚集索引。聚簇索引的文件存放在主键索引的叶子节点上，因此 InnoDB 必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。而 MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。


```

1. MyISAM不支持事务，InnoDB支持事务。由于MyISAM在很长一段时间内是MySQL的默认存储引擎，所以在很多人的印象中MySQL是不支持事务的数据库。实际上，InnoDB是一个性能良好的事务性引擎。它实现了四个标准的隔离级别，默认的隔离级别为可重复读（REPEATABLE READ），并通过间隙锁策略来防止幻读的出现。此外它还通过多版本并发控制（MVCC）来支持高并发。

2. 对表的行数查询的支持不同。MyISAM内置了一个计数器来存储表的行数。执行 select count(*) 时直接从计数器中读取，速度非常快。而InnoDB不保存这些信息，执行 select count(*)需要全表扫描。当表中数据量非常大的时候速度很慢。

3. 锁的粒度不同。MyISAM仅支持表锁。每次操作锁住整张表。这种处理方式一方面加锁的开销比较小，且不会出现死锁，但另一方面并发性能较差。InnoDB支持行锁。每次操作锁住一行数据，一方面行级锁在每次获取锁和释放锁的操作需要消耗比表锁更多的资源，速度较慢，且可能发生死锁，但是另一方面由于锁的粒度较小，发生锁冲突的概率也比较低，并发性较好。此外，即使是使用了InnoDB存储引擎，但如果MySQL执行一条sql语句时不能确定要扫描的范围，也会锁住整张表。

4. 对主键的要求不同。MyISAM允许没有主键的表存在。而如果在建表时没有显示的指定主键，InnoDB就会为每一行数据自动生成一个6字节的ROWID列，并以此做为主键。这种主键对用户不可见。InnoDB对主键采取这样的策略是与它的数据和索引的组织方式有关的，下文会讲到。

5. 数据和索引的组织方式不同。MyISAM将索引和数据分开进行存储。索引存放在.MYI文件中，数据存放在.MYD文件中。索引中保存了相应数据的地址。以表名+.MYI文件分别保存。 InnoDB的主键索引树的叶子节点保存主键和相应的数据。其它的索引树的叶子节点保存的是主键。也正是因为采取了这种存储方式，InnoDB才强制要求每张表都要有主键。

6. 对AUTO_INCREMENT的处理方式不一样。如果将某个字段设置为INCREMENT，InnoDB中规定必须包含只有该字段的索引。但是在MyISAM中，也可以将该字段和其他字段一起建立联合索引。

7. delete from table的处理方式不一样。MyISAM会重新建立表。InnoDB不会重新建立表，而是一行一行的删除。因此速度非常慢。推荐使用truncate table，不过需要用户有drop此表的权限。

8. MyISAM崩溃后无法安全恢复，InnoDB支持崩溃后的安全恢复。InnoDB实现了一套完善的崩溃恢复机制，保证在任何状态下(包括在崩溃恢复状态下)数据库挂了，都能正常恢复。

9. MyISAM不支持外键，InnoDB支持外键。

10. 缓存机制不同。MyISAM仅缓存索引信息，而不缓存实际的数据信息。而InnoDB不仅缓存索引信息，还会缓存数据信息。其将数据文件按页读取到缓冲池，然后按最近最少使用的算法来更新数据。

### 事务四大特性（ACID）

- 原子性（Atomicity）：要么全部被执行成功，要么就全部不被执行
- 一致性（Consistency）：事务执行以后，数据库从一种状态转换为另一种状态。
- 隔离性（Isolation）：事务在成功之前，处于隔离状态，事务不会受到外界的影响。
- 持久性（Durability）：事务成功之后，状态就永久保存，数据的状态不会发生改变，事务提交后出现其它错误，事务的处理结果也会得到保存。

### 三范式：

- 第一范式（1NF）：列具有原子性，一列不能再分成几列
- 第二范式（2NF）：表必须具有主键，如果有列没有包含在主键之中，那么这些列必须依赖于主键，并且不能只依赖于主键的一部分。
- 第三范式（3NF）：非主键列必须直接依赖于主键，不能使用传递依赖。比如以下间接依赖不被允许：非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键。

### SQL 语句主要有哪些？分别有什么作用？

- DQL：数据查询语言，查询数据，如 select
- DML：数据操作语言，增加、修改、删除数据，如 insert、udpate、delete
- TPL：事务处理语言，处理事务，包括 begin transaction、commit、rollback
- DCL：数据控制语言，进行授权与权限回收，如 grant、revoke
- DDL：数据定义语言，管理数据库和数据表，如 create、drop
- CCL：指针控制语言，通过指针对表进行操作，如 declare cursor

### 什么是索引？索引的优缺点是什么？

- 索引：索引是数据表的引用指针，它是表空间的一个组成部分。类似于一本书的目录。
- 索引目的：增加数据库的查询速度，索引类似一个目录，查询数据时候直接查询索引，大大提高查询的效率。
- 索引的优缺点：提高数据库查询速度，创建数据时候，也需要创建对应的索引，降低了数据写入的速度，此外建立大量的索引，也会占用大量的磁盘空间。

### 什么是视图？视图有什么作用？

- 视图是根据查询结果返回的一张虚拟的表，用于数据查询对比。
- 视图是一条 SELECT 语句被执行后返回的结果，返回的结果就是一张表，该表就是视图。
- 视图并会存储具体的数据，当数据库里面的数据发生变化以后，视图同时也会发生变化。

视图的作用：

- 视图可以简化用户的查询操作
- 视图可以方便的对比数据
- 视图为数据库提供了一定程度的逻辑独立性
- 视图能够对机密数据提供安全保护

### 如何进行 SQL 优化？

```
# 从搭建数据库的角度上来描述问题
# 建表的角度上
    # 1.合理安排表关系
    # 2.尽量把固定长度的字段放在前面
    # 3.尽量使用char代替varchar
    # 4.分表: 水平分,垂直分
# 使用sql语句的时候
    # 1.尽量用where来约束数据范围到一个比较小的程度,比如说分页的时候
    # 2.尽量使用连表查询而不是子查询
    # 3.删除数据或者修改数据的时候尽量要用主键作为条件
    # 4.合理的创建和使用索引
        # 1.查询的条件字段不是索引字段
            # 对哪一个字段创建了索引,就用这个字段做条件查询
        # 2.在创建索引的时候应该对区分度比较大的列进行创建
            # 1/10以下的重复率比较适合创建索引
        # 3.范围
            # 范围越大越慢
            # 范围越小越快
            # like 'a%'  快
            # like '%a'  慢
        # 4.条件列参与计算/使用函数
        # 5.and和or
            # id name
            # select * from s1 where id = 1800000 and name = 'eva';
            # select count(*) from s1 where id = 1800000 or name = 'eva';
            # 多个条件的组合,如果使用and连接
                # 其中一列含有索引,都可以加快查找速度
            # 如果使用or连接
                # 必须所有的列都含有索引,才能加快查找速度
        # 6.联合索引 : 最左前缀原则(必须带着最左边的列做条件,从出现范围开始整条索引失效)
            # (id,name,email)
            # select * from s1 where id = 1800000 and name = 'eva' and email = 'eva1800000@oldboy';
            # select * from s1 where id = 1800000 and name = 'eva';
            # select * from s1 where id = 1800000 and email = 'eva1800000@oldboy';
            # select * from s1 where id = 1800000;
            # select * from s1 where name = 'eva' and email = 'eva1800000@oldboy';
            # (email,id,name)
            # select * from s1 where id >10000 and email = 'eva1800000@oldboy';
        # 7.条件中写出来的数据类型必须和定义的数据类型一致
            # select * from biao where name = 666   # 不一致
        # 8.select的字段应该包含order by的字段
            # select name,age from 表 order by age;  # 比较好
            # select name from 表 order by age;  # 比较差

什么是最左前缀原则？什么是最左匹配原则
顾名思义，就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。
最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。
=和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式
```



## Redis

### Redis 常见数据类型有哪些？各自有什么应用场景？

- 常见的数据类型一共 5 种：String、Hash、List、Set、Sorted set
- String：最常用的数据类型，用于网站计数，排名，粉丝数量等
- Hash：field 和 value 的一个映射表，一般用于存储对象，常用于存储各种信息
- List：类似于列表，和列表一样支持查找和遍历，常用于消息列表，网站的分页也可以使用
- Set：set 功能和 List 类似，但是它可以排除重复的数据，并且和集合一样，可以进行交集、并集、差集的操作。比如可以用于 QQ 好友的 DNA 分析，是否有共同好友等等。
- Sorted set：有序集合，常用于网站的排行榜。

### Redis 的事务是什么？

NoSQL 数据库对事务支持性能不太完善，但是 Redis 支持原子操作和隔离操作。

Redis 通过 MULTI、EXEC、WATCH 等一系列的命令来实现事务（transaction）功能。

Redis 事务是一些列 Redis 命令的集合：它是一个单独的隔离操作，事物中的命令队列都会按顺序全部执行，不会被外来事物干扰；事物的执行具有原子性，事务中的命令要么全部执行成功，要么全部都不执行。

Redis 事物执行过程：开始事务、命令入队、执行事务。

### redis 有哪几种数据淘汰策略？

Redisn 内存淘汰，我们首先需要设置 server.maxmemory，该参数表示可以使用最大的内存。当内存达到设置值后就开始淘汰数据。

内存淘汰策略种类：

1. volatile-lru: 从设置了过期时间的数据中，选择近期最少使用数据进行删除
2. volatile-ttr: 从设置了过期时间的数据中，选择即将过期的数据进行删除
3. volatile-random: 从设置了过期时间的数据中，中随机选择数据进行删除
4. allkeys-lru: 从所有数据中，选择最少使用的数据进行删除，
5. allkeys-random: 从所有数据中，随机选择数据删除
6. no-eviction：严禁删除数据，新的数据写入时候没有空间，就会报错

### 如何解决 Redis 的并发竞争 Key 的问题？

- 大量请求同时取请求一个 key，导致最终的结果和我们预想的结果不同，就是并发竞争 Key。
- 针对并发竞争 key 的问题，实际开发使用分布式锁应对。推荐使用 ZooKeeper 和 Redis 来实现分布式锁。但是使用锁的缺点会降低 Redis 的性能。如果不存在竞争，最好不要使用。

### 什么是分布式锁？

分布式锁是解决并发访问共享资源的一种方法。分布式锁具有互斥性，同一时间只能有一个访问客户拥有锁，锁超时以后，会自动释放锁，防止出现死锁。锁设计时候要支持非阻塞和阻塞。

分布式锁的实现方案

- 数据库实现（乐观锁）
- 基于 ZooKeeper 的实现
- 基于 Redis 的实现（推荐）

### Python 如何实现一个 Redis 分布式锁

Redis 分布式锁应该具备哪些条件：

- 在分布式系统环境下，一个方法在同一时间只能被一个机器的一个线程执行
- 高可用的获取锁与释放锁
- 高性能的获取锁与释放锁
- 具备可重入特性
- 具备锁失效机制，防止死锁
- 具备非阻塞锁特性，即没有获取到锁将直接返回获取锁失败

选用 Redis 实现分布式锁原因：

- Redis 有很高的性能
- Redis 命令对此支持较好，实现起来比较方便

使用命令介绍：

SETNX

- SETNX key val：当且仅当 key 不存在时，set 一个 key 为 val 的字符串，返回 1；若 key 存在，则什么都不做，返回 0。

expire

- expire key timeout：为 key 设置一个超时时间，单位为 second，超过这个时间锁会自动释放，避免死锁。

delete

- delete key：删除 key, 在使用 Redis 实现分布式锁的时候，主要就会使用到这三个命令。

Redis 分布式锁实现思想（Redis 实现分布式锁主要使用 Redis 单线程的对 key 原子操作特性）：

- 获取锁的时候，使用 setnx 加锁，并使用 expire 命令为锁添加一个超时时间，超过该时间则自动释放锁，锁的 value 值为一个随机生成的 UUID（Universally Unique Identifier，翻译为中文是通用唯一识别码，UUID 的目的是让分布式系统中的所有元素都能有唯一的识别信息），通过此值在释放锁的时候进行判断。
- 获取锁的时候还设置一个获取的超时时间，若超过这个时间则放弃获取锁。
- 释放锁的时候，通过 UUID 判断是不是该锁，若是该锁，则执行 delete 进行锁释放。

```bash
import redis
import uuid
import time

class RedisLock():

    def __init__(self, lock_name, time_out, acquire_time):
        # 连接redis数据库
        self.redis_client = redis.StrictRedis(host='127.0.0.1', port=6379, db=0)
        self.lock_name = lock_name   # 锁的名称(锁的唯一ID)
        self.time_out = time_out  # 锁的超时时间(超过时间，自动释放锁)
        self.acquire_time = acquire_time  # 获取锁的时间(超过时间，放弃获取锁)

    def acquire_lock(self):
        '''获取一个分布式锁'''
        # 生成一个唯一的uuid值作为锁(键)的值(补充：UUID，Universally Unique Identifier，翻译为中文是通用唯一识别码，UUID 的目的是让分布式系统中的所有元素都能有唯一的识别信息)
        identifier = str(uuid.uuid4())
        end = time.time() + self.acquire_time
        lock = 'string:lock:' + self.lock_name
        while time.time() < end:
            # 设置一个锁，设置锁的名称和唯一的UUID值
            if self.redis_client.setnx(lock, identifier):
                # 给锁设置超时时间，防止进程崩溃导致其它进程无法获取锁
                self.redis_client.expire(lock, self.time_out)
                return identifier
            elif not self.redis_client.ttl(lock): # ttl获取锁的生存时间
                self.redis_client.expire(lock, self.time_out)
            time.sleep(0.001)
        return False

    def release_lock(self, identifier):
        """通用的锁释放函数"""
        lock = "string:lock:" + self.lock_name
        pip = self.redis_client.pipeline(True)
        while True:
            try:
                pip.watch(lock)
                # 获取锁的值，即设置锁时的UUID值
                lock_value = self.redis_client.get(lock)
                if not lock_value:
                    return True

                if lock_value.decode() == identifier:
                    pip.multi()
                    pip.delete(lock)
                    pip.execute()
                    return True
                pip.unwatch()
                break
            except redis.exceptions.WatchError:
                pass
        return False

if __name__ == '__main__':
    redis_lock = RedisLock(lock_name='lock001', time_out=10, acquire_time=10)
```

# 布隆过滤原理

布隆过滤器通过引入 K 个相互独立的哈希函数，将元素进行哈希后，然后进行元素判重的过程。

相对于一般算法，具有极高的空间效率和极快的查询效率，缺点是**有一定的误判率（极低）**和**删除困难**。

一般情况下不能从布隆过滤器中删除元素。我们很容易想到把位列阵变成整数数组，每插入一个元素相应的计数器加1, 这样删除元素时将计数器减掉就可以了。然而要保证安全的删除元素并非如此简单。首先我们必须保证删除的元素的确在布隆过滤器里面. 这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。

Bloom-Filter 核心思想是多个哈希函数，降低误判率，只要有一个 hash 值不在集合之中，就可以判断元素步骤集合之中。布隆过滤器一般用于大量数据去重判断，比如爬虫去重。

### **存入过程**

- 通过K个哈希函数计算该数据，返回K个计算出的hash值
- 这些K个hash值映射到对应的K个二进制的数组下标
- 将K个下标对应的二进制数据改成1。

# 设计模式

https://www.runoob.com/design-pattern/design-pattern-tutorial.html

# frida和xposed的区别

## xposed

https://blog.csdn.net/forward222/article/details/109237478

在Android系统中App进程都是由Zygote进程“孵化”出来的。Zygote进程在启动时会创建一个虚拟机实例，每当它“孵化”一个新的应用程序进程时，都会将这个Dalvik虚拟机实例复制到新的App进程里面去，从而使每个App进程都有一个独立的Dalvik虚拟机实例。

Zygote进程在启动的过程中，除了会创建一个虚拟机实例之外还会将Java Rumtime加载到进程中并注册一些Android核心类的JNI（Java Native Interface，Java本地接口）方法。一个App进程被Zygote进程孵化出来的时候，不仅会获得Zygote进程中的虚拟机实例拷贝，还会与Zygote进程一起共享Java Rumtime，也就是可以将XposedBridge.jar这个Jar包加载到每一个Android App进程中去。安装Xposed Installer之后，系统app_process将被替换，然后利用Java的Reflection机制覆写内置方法，实现功能劫持。

## frida

https://mp.weixin.qq.com/s/PDDMEkFtZ3Za9cl-oN90-Afrida

注入的主要思路就是找到目标进程,使用ptrace跟踪目标进程获取mmap，dlpoen，dlsym等函数库的便宜获取mmap在目标进程申请一段内存空间将在目标进程中找到存放[frida-agent-32/64.so]的空间启动执行各种操作由agent去实现。

补充：frida注入之后会在远端进程分配一段内存将agent拷贝过去并在目标进程中执行代码，执行完成后会 detach 目标进程，这也是为什么在 frida 先连接上目标进程后还可以用gdb/ida等调试器连接，而先gdb连接进程后 frida 就无法再次连上的原因(frida在注入时只会ptrace一下下注入完毕后就会结束ptrace所以ptrace占坑这种反调试使用spawn方式启动即可)。frida-agent 注入到目标进程并启动后会启动一个新进程与 host 进行通信，从而 host 可以给目标进行发送命令，比如执行代码，激活/关闭 hook，同时也能接收到目标进程的执行返回以及异步事件信息等。

# 排序

### 冒泡

```python
import random

from utils.decorator import cal_time
# li = [100, 89 , 98, 23, 76, 38, 85, 12, 9, 0, 4]
li = list(range(100))
random.shuffle(li)
# print(li)

# 时间复杂度n^2
@cal_time
def bubble_sort(li):
    for i in range(len(li) - 1):
        exchange = False
        for j in range(len(li) - i - 1):
            if li[j] > li[j + 1]:
                li[j], li[j + 1] = li[j + 1], li[j]
                exchange = True
        if not exchange:
            break


bubble_sort(li)
print(li)

```

### 插入

```python
import random

from utils.decorator import cal_time

"""
列表分为有序区和无序区域(这俩个区域都在同一个列表进行操作)，俩个部分，最初有序的区域只要一个元素
每次从无序的区域哪一个插入到有序的区域的位置，直到最后
"""
li = [100, 2, 89, 98, 23, 76, 38, 85, 12, 9, 4]


# 时间复杂度O(n^2)
@cal_time
def insert_sort(li):
    for i in range(1, len(li)):  # 摸到的牌的下标从1开始，代表摸到牌的下标，第0个默认是最开始的有序的区域
        tmp = li[i]  # 摸到的牌的值，先存起来
        j = i - 1  # 代表的是剩下的牌的下标
        # 往后移动牌的条件：拿到的牌要比待插入的有序的区域中的值小。j>=0表示的已经到有序区域最小值，再往前走没有比较的值了
        while j >= 0 and tmp < li[j]:
            li[j + 1] = li[j]
            j -= 1
        # 在循环结束的位置，要么-1（已经到头了），要么li[j]（有序区域的值都比摸到的牌的值小，就是说你不用插入，往后插入）
        li[j + 1] = tmp

li = list(range(10000))
random.shuffle(li)
# print(li)
insert_sort(li)
print(li)

```

### 选择排序

```python

li = [100, 2, 89, 98, 23, 76, 38, 85, 12, 9, 4]

"""
获取最小值的位置
"""


def get_min_pos(li):
    min_pos = 0
    for i in range(1, len(li)):
        if li[i] < li[min_pos]:
            min_pos = i
    return li[min_pos]


def select_sort(li):
    for i in range(len(li) - 1):
        min_pos = i
        for j in range(i + 1, len(li)):
            if li[j] < li[min_pos]:
                min_pos = j
        li[i], li[min_pos] = li[min_pos], li[i]


select_sort(li)
print(li)

```



### 希尔排序

```
import random
from utils.decorator import cal_time


def insert_sort(li, d):
    for i in range(d, len(li)):  # 摸到的牌的下标从1开始，代表摸到牌的下标，第0个默认是最开始的有序的区域
        tmp = li[i]  # 摸到的牌的值，先存起来
        j = i - d  # 代表的是剩下的牌的下标
        # 往后移动牌的条件：拿到的牌要比待插入的有序的区域中的值小。j>=0表示的已经到有序区域最小值，再往前走没有比较的值了
        while j >= 0 and tmp < li[j]:
            li[j + d] = li[j]
            j -= d
        # 在循环结束的位置，要么-1（已经到头了），要么li[j]（有序区域的值都比摸到的牌的值小，就是说你不用插入，往后插入）
        li[j + d] = tmp

@cal_time
def shell_sort(li):
    d = len(li) // 2
    while d > 0:
        insert_sort(li, d)
        d = d // 2


li = list(range(10000))
random.shuffle(li)
# print(li)
shell_sort(li)
# print(li)

```

### 快速排序

```python
li = [100, 2, 89, 98, 23, 76, 38, 85, 12, 9, 4]


def quick_sort2(li):
    if len(li) < 2:
        return li
    tmp = li[0]
    left_list = [x for x in li[1:] if x <= tmp]
    right_list = [x for x in li[1:] if x > tmp]

    return quick_sort2(left_list) + [tmp] + quick_sort2(right_list)


li = quick_sort2(li)
print(li)

def quick_sort(li, left, right):
    """
    注意：快速排序是直接在原始数组里进行各种交换操作，所以当子数组被分割出来的时候，原始数组里的排列也被改变了
    :param li: 所要排序的列表
    :param left: 排序的区域的
    :param right: 排序的区域的
    :return:
    """

    if left < right and len(li) >= 2:
        mid = partition1(li, left, right)
        quick_sort(li, left, mid - 1)
        quick_sort(li, mid + 1, right)


def partition(li, left, right):
    """
    1.找一个基准值（可以是固定，可以是随机的）
    2.根据这个基准值，分成俩部分，一部分比他大的放右边，比他小的放左边
    3.最后把这个基准值放回到空缺的位置，然后返回
    :param li:
    :param left:
    :param right:
    :return:
    """
    tmp = li[left]
    while left < right:
        while left < right and li[right] >= tmp:
            right -= 1
        li[left] = li[right]
        while left < right and li[left] <= tmp:
            left += 1
        li[right] = li[left]
    li[left] = tmp
    return left

```

### 归并排序

```python

li1 = [15, 17, 18, 20, 10, 11, 13, 14, 19]
li2 = [100, 2, 89, 98, 23, 76, 38, 85, 12, 9, 4]


def merge(li, low, mid, high):
    """
    将俩个排序好的列表，归并在一个，且排好顺序
    :param li:
    :param low:
    :param mid:
    :param high:
    :return:
    """
    i = low
    j = mid + 1
    li_tmp = []
    while i <= mid and j <= high:
        if li[i] <= li[j]:  # 排好序的左边和右边比较。如果小的先放入临时的列表
            li_tmp.append(li[i])
            i += 1  # 往后移动
        else:
            li_tmp.append(li[j])
            j += 1
    while i <= mid:
        li_tmp.append(li[i])
        i += 1
    while j <= high:
        li_tmp.append(li[j])
        j += 1
    for i in range(low, high + 1):
        print(i)
        li[i] = li_tmp[i - low]


def merege_sort(li, low, high):
    """
    递归的思想解决问题，类似汉罗塔问题
    :param li: 待排序的列表
    :param low:
    :param high:
    :return:
    """
    if low < high:
        mid = (low + high) // 2
        merege_sort(li, low, mid)
        merege_sort(li, mid + 1, high)
        merge(li, low, mid, high)


# merege(li1, 0, 4, 8)
# print(li1)


merege_sort(li2, 0, len(li2) - 1)
print(li2)

```

### 堆排序

```python
def shift_down(li, low, high):
    """
    堆的调整(大根堆)
    :param li: 树的根
    :param low: 树的根的位置
    :param high: 树的最后的一个节点的位置，表示数的范围
    :return:
    """
    tmp = li[low]  # 把根的值用临时变量先存储
    i = low  # 拿出来之后，空位的位置
    j = 2 * i + 1  # 俩个孩子的位置
    while j <= high:  # 循环退出的第二个条件：j>high，说米i是叶子节点
        if j + 1 <= high and li[j] < li[j + 1]:  # 右孩子必须存在才能j+1
            j += 1
        if li[j] > tmp:
            li[i] = li[j]
            i = j
            j = 2 * i + 1
        else:  # 循环退出的第一个条件：j位置的值比tmp小，说明俩个孩子都比tmp小
            break
    li[i] = tmp



# li = [2, 9, 7, 8, 5, 0, 1, 6, 4, 3]
# shift_down(li, 0, len(li)-1)
# print(li)
#

def build_heap(li):
    """
    堆排序
    :return:
    """
    n = len(li)
    # 1.从最后的节点位置开始构造堆，low = (n - 1 - 1) // 2, high = n - 1
    for low in range(n // 2 - 1, -1, -1):
        shift_down(li, low, n - 1)

@cal_time
def heap_sort(li):
    n = len(li)
    build_heap(li)
    for high in range(n - 1, -1, -1):
        li[0], li[high] = li[high], li[0]
        shift_down(li, 0, high - 1)

li = list(range(10000))
random.shuffle(li)
heap_sort(li)
# print(li)

```

·